{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most repeated words in Romeo and Juliet text are: [('br', 2893), ('p', 2119), ('class', 1217), ('drama', 874), ('the', 782), ('I', 583), ('a', 580), ('and', 551), ('to', 535), ('of', 479)]\n"
     ]
    }
   ],
   "source": [
    "# Day 20 of 30 Days of Python\n",
    "\n",
    "# Read this url and find the 10 most frequent words. romeo_and_juliet = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "import requests\n",
    "\n",
    "# Find the 10 most repeated words in the romeo_and_juliet.txt\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_repeated_words(url, num_words):\n",
    "\n",
    "    response = requests.get(url)\n",
    "    text = response.text\n",
    "    # Tokenize the text into words\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "    # Count the frequency of each word\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    # Get most common words and their frequencies\n",
    "    most_repeated_words = word_counts.most_common(num_words)\n",
    "\n",
    "    return most_repeated_words\n",
    "\n",
    "# Function call to return most repeated words\n",
    "result = find_most_repeated_words('https://www.gutenberg.org/files/1513/1513-h/1513-h.htm', 10)\n",
    "print(\"Most repeated words in Romeo and Juliet text are:\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY STATISTICS FOR CAT WEIGHT\n",
      "Min Weight: 3.0 kg\n",
      "Max Weight: 7.5 kg\n",
      "Mean Weight: 4.71 kg\n",
      "Median Weight: 4.5 kg\n",
      "Standard Deviation of Weight: 1.06 kg\n",
      "\n",
      "SUMMARY STATISTICS FOR CAT LIFESPAN\n",
      "Min Lifespan: 10.5 years\n",
      "Max Lifespan: 19.0 years\n",
      "Mean Lifespan: 13.75 years\n",
      "Median Lifespan: 13.5 years\n",
      "Standard Deviation of Lifespan: 1.57 kg\n",
      "\n",
      "FREQUENCY TABLE\n",
      "Egypt - Abyssinian: 1 times\n",
      "Greece - Aegean: 1 times\n",
      "United States - American Bobtail: 1 times\n",
      "United States - American Curl: 1 times\n",
      "United States - American Shorthair: 1 times\n",
      "United States - American Wirehair: 1 times\n",
      "United Arab Emirates - Arabian Mau: 1 times\n",
      "Australia - Australian Mist: 1 times\n",
      "United States - Balinese: 1 times\n",
      "United States - Bambino: 1 times\n",
      "United States - Bengal: 1 times\n",
      "France - Birman: 1 times\n",
      "United States - Bombay: 1 times\n",
      "United Kingdom - British Longhair: 1 times\n",
      "United Kingdom - British Shorthair: 1 times\n",
      "Burma - Burmese: 1 times\n",
      "United Kingdom - Burmilla: 1 times\n",
      "United States - California Spangled: 1 times\n",
      "United States - Chantilly-Tiffany: 1 times\n",
      "France - Chartreux: 1 times\n",
      "Egypt - Chausie: 1 times\n",
      "United States - Cheetoh: 1 times\n",
      "United States - Colorpoint Shorthair: 1 times\n",
      "United Kingdom - Cornish Rex: 1 times\n",
      "Canada - Cymric: 1 times\n",
      "Cyprus - Cyprus: 1 times\n",
      "United Kingdom - Devon Rex: 1 times\n",
      "Russia - Donskoy: 1 times\n",
      "China - Dragon Li: 1 times\n",
      "Egypt - Egyptian Mau: 1 times\n",
      "Burma - European Burmese: 1 times\n",
      "United States - Exotic Shorthair: 1 times\n",
      "United Kingdom - Havana Brown: 1 times\n",
      "United States - Himalayan: 1 times\n",
      "Japan - Japanese Bobtail: 1 times\n",
      "United States - Javanese: 1 times\n",
      "Thailand - Khao Manee: 1 times\n",
      "Thailand - Korat: 1 times\n",
      "Russia - Kurilian: 1 times\n",
      "Thailand - LaPerm: 1 times\n",
      "United States - Maine Coon: 1 times\n",
      "United Kingdom - Malayan: 1 times\n",
      "Isle of Man - Manx: 1 times\n",
      "United States - Munchkin: 1 times\n",
      "United States - Nebelung: 1 times\n",
      "Norway - Norwegian Forest Cat: 1 times\n",
      "United States - Ocicat: 1 times\n",
      "United States - Oriental: 1 times\n",
      "Iran (Persia) - Persian: 1 times\n",
      "United States - Pixie-bob: 1 times\n",
      "United States - Ragamuffin: 1 times\n",
      "United States - Ragdoll: 1 times\n",
      "Russia - Russian Blue: 1 times\n",
      "United States - Savannah: 1 times\n",
      "United Kingdom - Scottish Fold: 1 times\n",
      "United States - Selkirk Rex: 1 times\n",
      "Thailand - Siamese: 1 times\n",
      "Russia - Siberian: 1 times\n",
      "Singapore - Singapura: 1 times\n",
      "United States - Snowshoe: 1 times\n",
      "Somalia - Somali: 1 times\n",
      "Canada - Sphynx: 1 times\n",
      "Canada - Tonkinese: 1 times\n",
      "United States - Toyger: 1 times\n",
      "Turkey - Turkish Angora: 1 times\n",
      "Turkey - Turkish Van: 1 times\n",
      "United States - York Chocolate: 1 times\n"
     ]
    }
   ],
   "source": [
    "# Read the cats API and cats_api = 'https://api.thecatapi.com/v1/breeds' and find :\n",
    "# (i) the min, max, mean, median, standard deviation of cats' weight in metric units.\n",
    "# (ii) the min, max, mean, median, standard deviation of cats' lifespan in years.\n",
    "# (iii) Create a frequency table of country and breed of cats\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "# Define the Cat API URL\n",
    "cats_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "\n",
    "# Fetch data from the Cat API\n",
    "response = requests.get(cats_api)\n",
    "breeds_data = response.json()\n",
    "\n",
    "# Extract weights from the API response in metric unit\n",
    "weights = []\n",
    "\n",
    "for breed in breeds_data:\n",
    "    weight_str = breed.get('weight', {}).get('metric', '')\n",
    "    \n",
    "    # Handle weight ranges (e.g., '3 - 5' kg)\n",
    "    if '-' in weight_str:\n",
    "        # Take the average of the range\n",
    "        weight_values = [float(value.strip()) for value in weight_str.split('-')]\n",
    "        avg_weight = sum(weight_values) / len(weight_values)\n",
    "        weights.append(avg_weight)\n",
    "    else:\n",
    "        # Convert single values to float\n",
    "        weights.append(float(weight_str) if weight_str else 0.0)\n",
    "        \n",
    "# Calculate statistics\n",
    "min_weight = min(weights)\n",
    "max_weight = max(weights)\n",
    "mean_weight = np.mean(weights).round(2)\n",
    "median_weight = np.median(weights)\n",
    "std_dev_weight = np.std(weights).round(2)\n",
    "\n",
    "# Display the results\n",
    "print(\"SUMMARY STATISTICS FOR CAT WEIGHT\")\n",
    "print(f\"Min Weight: {min_weight} kg\")\n",
    "print(f\"Max Weight: {max_weight} kg\")\n",
    "print(f\"Mean Weight: {mean_weight} kg\")\n",
    "print(f\"Median Weight: {median_weight} kg\")\n",
    "print(f\"Standard Deviation of Weight: {std_dev_weight} kg\")\n",
    "\n",
    "# Extract lifespan from the API response\n",
    "lifespan = []\n",
    "\n",
    "for breed in breeds_data:\n",
    "    lifespan_str = breed.get('life_span','')\n",
    "    \n",
    "    # Handle lifespan ranges (e.g., '3 - 5' years)\n",
    "    if '-' in lifespan_str:\n",
    "        # Take the average of the range\n",
    "        lifespan_values = [float(value.strip()) for value in lifespan_str.split('-')]\n",
    "        avg_lifespan = sum(lifespan_values) / len(lifespan_values)\n",
    "        lifespan.append(avg_lifespan)\n",
    "    else:\n",
    "        # Convert single values to float\n",
    "        lifespan.append(float(lifespan_str) if lifespan_str else 0.0)\n",
    "        \n",
    "# Calculate statistics\n",
    "min_lifespan = min(lifespan)\n",
    "max_lifespan = max(lifespan)\n",
    "mean_lifespan = np.mean(lifespan).round(2)\n",
    "median_lifespan = np.median(lifespan)\n",
    "std_dev_lifespan = np.std(lifespan).round(2)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nSUMMARY STATISTICS FOR CAT LIFESPAN\")\n",
    "print(f\"Min Lifespan: {min_lifespan} years\")\n",
    "print(f\"Max Lifespan: {max_lifespan} years\")\n",
    "print(f\"Mean Lifespan: {mean_lifespan} years\")\n",
    "print(f\"Median Lifespan: {median_lifespan} years\")\n",
    "print(f\"Standard Deviation of Lifespan: {std_dev_lifespan} kg\")\n",
    "\n",
    "# Create a frequency table for country and breed\n",
    "frequency_table = {}\n",
    "\n",
    "for breed in breeds_data:\n",
    "    country = breed.get('origin', 'Unknown')\n",
    "    breed_name = breed.get('name', 'Unknown')\n",
    "\n",
    "    # Increment the count in the frequency table\n",
    "    key = f\"{country} - {breed_name}\"\n",
    "    frequency_table[key] = frequency_table.get(key, 0) + 1\n",
    "\n",
    "# Display the frequency table\n",
    "print(\"\\nFREQUENCY TABLE\")\n",
    "for key, count in frequency_table.items():\n",
    "    print(f\"{key}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Largest Countries:\n",
      "1. Russia: 17098242.0 square kilometers\n",
      "2. Antarctica: 14000000.0 square kilometers\n",
      "3. Canada: 9984670.0 square kilometers\n",
      "4. China: 9706961.0 square kilometers\n",
      "5. United States: 9372610.0 square kilometers\n",
      "6. Brazil: 8515767.0 square kilometers\n",
      "7. Australia: 7692024.0 square kilometers\n",
      "8. India: 3287590.0 square kilometers\n",
      "9. Argentina: 2780400.0 square kilometers\n",
      "10. Kazakhstan: 2724900.0 square kilometers\n",
      "\n",
      "Top 10 Most Spoken Languages:\n",
      "1. English: 91 countries\n",
      "2. French: 46 countries\n",
      "3. Arabic: 25 countries\n",
      "4. Spanish: 24 countries\n",
      "5. Portuguese: 10 countries\n",
      "6. Dutch: 7 countries\n",
      "7. Russian: 7 countries\n",
      "8. German: 6 countries\n",
      "9. Chinese: 5 countries\n",
      "10. Swahili: 4 countries\n",
      "\n",
      "Total number of languages: 155\n"
     ]
    }
   ],
   "source": [
    "# Read the countries from https://restcountries.com/v3.1/all and find\n",
    "# (i) the 10 largest countries\n",
    "# (ii) the 10 most spoken languages\n",
    "# (iii) the total number of languages in the countries API\n",
    "\n",
    "# Define the URL for the Restcountries API\n",
    "restcountries_api = 'https://restcountries.com/v3.1/all'\n",
    "\n",
    "# Fetch data from the Restcountries API\n",
    "response = requests.get(restcountries_api)\n",
    "countries_data = response.json()\n",
    "\n",
    "# Extract relevant information (name and area) from the API response\n",
    "countries_info = [(country.get('name', {}).get('common', 'Unknown'), country.get('area', 0)) for country in countries_data]\n",
    "\n",
    "# Sort countries based on area in descending order\n",
    "sorted_countries = sorted(countries_info, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the 10 largest countries\n",
    "print(\"Top 10 Largest Countries:\")\n",
    "for i, (country_name, area) in enumerate(sorted_countries[:10], start=1):\n",
    "    print(f\"{i}. {country_name}: {area} square kilometers\")\n",
    "\n",
    "# Extract language information from each country\n",
    "all_languages = [country.get('languages', {}) for country in countries_data]\n",
    "\n",
    "# Flatten the list of languages\n",
    "all_language_names = [language for languages in all_languages for language in languages.values()]\n",
    "\n",
    "# Count the occurrences of each language\n",
    "language_counts = {language: all_language_names.count(language) for language in set(all_language_names)}\n",
    "\n",
    "# Sort languages based on the number of occurrences in descending order\n",
    "sorted_languages = sorted(language_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the 10 most spoken languages\n",
    "print(\"\\nTop 10 Most Spoken Languages:\")\n",
    "for i, (language_name, count) in enumerate(sorted_languages[:10], start=1):\n",
    "    print(f\"{i}. {language_name}: {count} countries\")\n",
    "\n",
    "# Collect all unique languages from the countries\n",
    "all_languages = set()\n",
    "\n",
    "for country in countries_data:\n",
    "    languages = country.get('languages', [])\n",
    "    all_languages.update(languages)\n",
    "\n",
    "# Print the total number of unique languages\n",
    "total_languages = len(all_languages)\n",
    "print(f\"\\nTotal number of languages: {total_languages}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the page: UCI Machine Learning Repository\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "UCI Machine Learning Repository\n",
      "\n",
      "Datasets - UCI Machine Learning Repository\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "       Datasets Contribute Dataset Donate New Link External About Us Who We Are Citation Metadata Contact Information           Login    Filters            Keywords     Data Type      Subject Area      Task      # Features      # Instances      Feature Type      Python    Browse Datasets   Filters  Sort by # Views, desc # Views   Name  # Instances  # Features  Date Donated  Relevance        Expand All Collapse All    Iris A small classic dataset from Fisher, 1936. One of the earliest known datasets used for evaluating classification methods.\n",
      "  Classification  Tabular  150 Instances  4 Features      Heart Disease 4 databases: Cleveland, Hungary, Switzerland, and the VA Long Beach  Classification  Multivariate  303 Instances  13 Features      Adult Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset.   Classification  Multivariate  48.84K Instances  14 Features      Dry Bean Dataset Images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. A total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.  Classification  Multivariate  13.61K Instances  16 Features      Wine Using chemical analysis to determine the origin of wines  Classification  Tabular  178 Instances  13 Features      Breast Cancer Wisconsin (Diagnostic) Diagnostic Wisconsin Breast Cancer Database.  Classification  Multivariate  569 Instances  30 Features      Diabetes This diabetes dataset is from AIM '94  Classification  Multivariate, Time-Series  1 Instances  20 Features      Wine Quality Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests (see [Cortez et al., 2009], http://www3.dsi.uminho.pt/pcortez/wine/).  Classification, Regression  Multivariate  4.9K Instances  12 Features      Car Evaluation Derived from simple hierarchical decision model, this database may be useful for testing constructive induction and structure discovery methods.  Classification  Multivariate  1.73K Instances  6 Features      Bank Marketing The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).  Classification  Multivariate  45.21K Instances  17 Features    Rows per page 510152025 0 to 10 of 663      Filters            Keywords     Data Type      Subject Area      Task      # Features      # Instances      Feature Type      Python     By using the UCI Machine Learning Repository,\n",
      "you acknowledge and accept the cookies and privacy practices used by the UCI Machine Learning Repository. Accept Read Policy  The Project About Us CML National Science Foundation Navigation Home View Datasets Donate a Dataset Logistics Contact Privacy Notice Feature Request or Bug Report  Browse Datasets Donate a Dataset Link an external Dataset  Who We Are Citation Metadata Contact Information   Login \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL for UCI Machine Learning Repository\n",
    "uci_url = 'https://archive.ics.uci.edu/ml/datasets'\n",
    "\n",
    "# Make an HTTP request to the UCI website\n",
    "response = requests.get(uci_url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Print the title of the page\n",
    "    title = soup.title.string.strip()\n",
    "    print(f\"Title of the page: {title}\\n\")\n",
    "\n",
    "    # Print the content of the page (this will be a large output)\n",
    "    print(soup.get_text())\n",
    "else:\n",
    "    print(f\"Failed to retrieve the content. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
