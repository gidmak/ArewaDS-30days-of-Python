{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and stored in facts_data.json\n"
     ]
    }
   ],
   "source": [
    "# Day 22 of 30 Days of Python Exercise\n",
    "# Scrape the following website and store the data as json file(url = 'http://www.bu.edu/president/boston-university-facts-stats/').\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = 'http://www.bu.edu/president/boston-university-facts-stats/'\n",
    "\n",
    "# Get data from the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Get all contents of the URL\n",
    "content = response.content\n",
    "\n",
    "# Create BeautifulSoup object\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "facts_section = soup.find('section', {'class': 'facts-categories'})\n",
    "\n",
    "# Check if the facts_section is found\n",
    "if facts_section:\n",
    "    data = {}\n",
    "\n",
    "    # Find all elements with class 'facts-wrapper' within facts_section\n",
    "    for wrapper in facts_section.find_all(class_='facts-wrapper'):\n",
    "        category = wrapper.find('h5').text.strip()\n",
    "        values = {}\n",
    "\n",
    "        # Find all list items within the current wrapper\n",
    "        for item in wrapper.find_all('li', class_='list-item'):\n",
    "            label = item.find('p', class_='text').text.strip()\n",
    "            value = item.find('span', class_='value').text.strip()\n",
    "            values[label] = value\n",
    "\n",
    "        data[category] = values\n",
    "\n",
    "    # Store the data as a JSON file\n",
    "    output_file = 'facts_data.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f'Data has been scraped and stored in {output_file}')\n",
    "else:\n",
    "    print('Unable to find the facts-stats section on the webpage.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and stored as adult_dataset.json.\n"
     ]
    }
   ],
   "source": [
    "# Extract the table in this url (https://archive.ics.uci.edu/ml/datasets.php) and change it to a json file\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/dataset/2/adult\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "# Checking (status code 200 means successful)\n",
    "if response.status_code == 200:\n",
    "    # Parsing the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Finding the table on the page\n",
    "    table = soup.find('table')\n",
    "\n",
    "    # Extraction of data from the table\n",
    "    data = []\n",
    "    headers = [header.text.strip() for header in table.find_all('th')]\n",
    "    rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "\n",
    "    for row in rows:\n",
    "        row_data = [cell.text.strip() for cell in row.find_all('td')]\n",
    "        data.append(dict(zip(headers, row_data)))\n",
    "\n",
    "    # Storing the data as a JSON file\n",
    "    output_file = 'adult_dataset.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Data has been scraped and stored as {output_file}.\")\n",
    "else:\n",
    "    print(\"Error: Unable to fetch the webpage. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been scraped and stored in presidents_data.json\n"
     ]
    }
   ],
   "source": [
    "# Scrape the presidents table and store the data as json(https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States). The table is not very structured and the scrapping may take very long time.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_presidents_of_the_United_States'\n",
    "\n",
    "# Get data from the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the presidents table\n",
    "presidents_table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Check if the table is found\n",
    "if presidents_table:\n",
    "    data = []\n",
    "\n",
    "    # Extract rows from the table\n",
    "    rows = presidents_table.find_all('tr')\n",
    "\n",
    "    # Extract data from each row\n",
    "    for row in rows[1:]:  # Start from the second row to skip the header\n",
    "        columns = row.find_all(['th', 'td'])\n",
    "\n",
    "        # Ensure the row has the expected number of columns\n",
    "        if len(columns) == 8:\n",
    "            row_data = {\n",
    "                \"Number\": columns[0].text.strip(),\n",
    "                \"Portrait\": columns[1].find('img')['src'] if columns[1].find('img') else None,\n",
    "                \"Name\": columns[2].text.strip(),\n",
    "                \"Term\": columns[3].text.strip(),\n",
    "                \"Party\": columns[5].text.strip(),\n",
    "                \"Election\": columns[6].text.strip(),\n",
    "                \"Vice President\": columns[7].text.strip(),\n",
    "            }\n",
    "\n",
    "            data.append(row_data)\n",
    "\n",
    "    # Store the data as a JSON file\n",
    "    output_file = 'presidents_data.json'\n",
    "    with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    print(f'Data has been scraped and stored in {output_file}')\n",
    "else:\n",
    "    print('Unable to find the presidents table on the webpage.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
